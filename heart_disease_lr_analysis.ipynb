{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heart Disease Risk Prediction: Logistic Regression \n",
    "## Step 1: Load and prepare the Dataset\n",
    "\n",
    "The hearth disease is one of the most important problems in the heathly area, that situation create the neccessity to search a solution or tratements for this problem, for that reason in kaggle exist information about predition or analizys of hearth desease. In this lab, i'm gonna explain this information and create an analysis with logistic regression.\n",
    "\n",
    "In the first section focuses on: Prepare the dataset and exploring the data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as pit\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos = pd.read_csv(\"heath.csv\")\n",
    "datos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Variable Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos['target'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "datos['target'].value_counts().plot(kind='bar')\n",
    "plt.title(\"Class Distribution (Heart Disease Presence)\")\n",
    "plt.xlabel(\"Target\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['age', 'chol', 'trestbps', 'thalach', 'oldpeak', 'ca']\n",
    "X = datos[features].values\n",
    "y = datos['target'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "promedio_X = X.mean(axis=0)\n",
    "desviacion_X = X.std(axis=0)\n",
    "\n",
    "X_norm = (X - promedio_X) / desviacion_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression converges faster when features are normalized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split 70/30 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_muestras = len(y)\n",
    "indices = np.arange(total_muestras)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "tam_entrenamiento = int(0.7 * total_muestras)\n",
    "\n",
    "idx_train = indices[:tam_entrenamiento]\n",
    "idx_test = indices[tam_entrenamiento:]\n",
    "\n",
    "X_train = X_norm[idx_train]\n",
    "y_train = y[idx_train]\n",
    "\n",
    "X_test = X_norm[idx_test]\n",
    "y_test = y[idx_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verification of rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train disease rate:\", y_train.mean())\n",
    "print(\"Test disease rate:\", y_test.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the values are same, we can say the split is complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 Summary\n",
    "\n",
    "- Dataset downloaded from Kaggle (Heart Disease Dataset)\n",
    "- 303 patient records with clinical features\n",
    "- Target variable indicates presence (1) or absence (0) of heart disease\n",
    "- Selected 6 clinically relevant features\n",
    "- No missing values detected\n",
    "- Features normalized for gradient descent optimization\n",
    "- Data split into 70% training and 30% testing sets\n",
    "\n",
    "The dataset is now ready for implementing logistic regression from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Implement Basic Logistic Regression\n",
    "## Objective of model\n",
    "Predict the probability of heart disease in the pacients given a set of clinics values using a logistic regression, without libraries of high level ML.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Sigmoid activation function\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the model is a probability between 0 and 1, we can see that like a estimated risk of heart disease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We using a Binary Cross Entropy, it a standard in binary clasification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, pesos, sesgo):\n",
    "    \"\"\"\n",
    "    Binary cross-entropy loss\n",
    "    \"\"\"\n",
    "    num_ejemplos = X.shape[0]\n",
    "    z = X @ pesos + sesgo\n",
    "    predicciones = sigmoid(z)\n",
    "    \n",
    "    epsilon = 1e-8  # numerical stability\n",
    "    costo = -(1/num_ejemplos) * np.sum(\n",
    "        y * np.log(predicciones + epsilon) +\n",
    "        (1 - y) * np.log(1 - predicciones + epsilon)\n",
    "    )\n",
    "    return costo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This funtion penalizes heavily:\n",
    "- False negatives (risk not detected)\n",
    "- False predictions very relible but incorrects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(X, y, pesos, sesgo):\n",
    "    \"\"\"\n",
    "    Compute gradients of cost w.r.t w and b\n",
    "    \"\"\"\n",
    "    num_ejemplos = X.shape[0]\n",
    "    predicciones = sigmoid(X @ pesos + sesgo)\n",
    "    \n",
    "    grad_pesos = (1/num_ejemplos) * (X.T @ (predicciones - y))\n",
    "    grad_sesgo = (1/num_ejemplos) * np.sum(predicciones - y)\n",
    "    \n",
    "    return grad_pesos, grad_sesgo\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, tasa_aprendizaje=0.01, iteraciones=2000):\n",
    "    \"\"\"\n",
    "    Train logistic regression using gradient descent\n",
    "    \"\"\"\n",
    "    num_ejemplos, num_features = X.shape\n",
    "    pesos = np.zeros(num_features)\n",
    "    sesgo = 0.0\n",
    "    \n",
    "    historial_costo = []\n",
    "    \n",
    "    for i in range(iteraciones):\n",
    "        grad_pesos, grad_sesgo = compute_gradients(X, y, pesos, sesgo)\n",
    "        \n",
    "        pesos -= tasa_aprendizaje * grad_pesos\n",
    "        sesgo -= tasa_aprendizaje * grad_sesgo\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            costo = compute_cost(X, y, pesos, sesgo)\n",
    "            historial_costo.append(costo)\n",
    "    \n",
    "    return pesos, sesgo, historial_costo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If alpha is near to 0.01, it is a great point to start.\n",
    "- 1000 - 3000 iterations ensure stable convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pesos, sesgo, historial_costo = gradient_descent(\n",
    "    X_train, y_train,\n",
    "    tasa_aprendizaje=0.01,\n",
    "    iteraciones=2500\n",
    ")\n",
    "\n",
    "print(\"Final weights:\", pesos)\n",
    "print(\"Final bias:\", sesgo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(historial_costo)\n",
    "plt.xlabel(\"Iterations (x50)\")\n",
    "plt.ylabel(\"Cost (Binary Cross-Entropy)\")\n",
    "plt.title(\"Training Loss Convergence\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When we can get a slowly decrease, the GD is stable.\n",
    "- Not oscillations means learning rate is great.\n",
    "- If the model get a convergence means the model learned a reasonable border."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, pesos, sesgo, umbral=0.5):\n",
    "    \"\"\"\n",
    "    Predict class labels using learned parameters\n",
    "    \"\"\"\n",
    "    probabilidades = sigmoid(X @ pesos + sesgo)\n",
    "    return (probabilidades >= umbral).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_metrics(y_real, y_pred):\n",
    "    exactitud = np.mean(y_real == y_pred)\n",
    "    \n",
    "    tp = np.sum((y_real == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_real == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_real == 1) & (y_pred == 0))\n",
    "    \n",
    "    precision = tp / (tp + fp + 1e-8)\n",
    "    recall = tp / (tp + fn + 1e-8)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "    \n",
    "    return exactitud, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = predict(X_train, pesos, sesgo)\n",
    "y_test_pred = predict(X_test, pesos, sesgo)\n",
    "\n",
    "train_metrics = classification_metrics(y_train, y_train_pred)\n",
    "test_metrics = classification_metrics(y_test, y_test_pred)\n",
    "\n",
    "print(\"Train metrics (Acc, Prec, Recall, F1):\", train_metrics)\n",
    "print(\"Test metrics  (Acc, Prec, Recall, F1):\", test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clinical interpretation of results\n",
    "\n",
    "- Accuracy: general correctness\n",
    "\n",
    "- Recall: ability to detect patients with heart disease (critical)\n",
    "\n",
    "- Precision: reliability of positive predictions\n",
    "\n",
    "- F1: balance between false positives and false negatives\n",
    "\n",
    "In medical risk prediction, recall is often more important than accuracy, as missing a high-risk patient is more costly than a false alarm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
