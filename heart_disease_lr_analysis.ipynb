{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heart Disease Risk Prediction: Logistic Regression \n",
    "- By: Juan Jose Mejia Celis\n",
    "## Step 1: Load and prepare the Dataset\n",
    "\n",
    "Heart disease is a major cause of death globally. Machine learning models can help identify at-risk patients by analyzing clinical features. This notebook uses logistic regression on the Kaggle Heart Disease Dataset to predict disease presence.\n",
    "\n",
    "First, we prepare the data and do some exploratory analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"heart.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Variable Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['target'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "data['target'].value_counts().plot(kind='bar')\n",
    "plt.title(\"Class Distribution (Heart Disease Presence)\")\n",
    "plt.xlabel(\"Target\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['age', 'chol', 'trestbps', 'thalach', 'oldpeak', 'ca']\n",
    "X = data[features].values\n",
    "y = data['target'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_X = X.mean(axis=0)\n",
    "std_X = X.std(axis=0)\n",
    "\n",
    "X_norm = (X - mean_X) / std_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression converges faster when features are normalized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split 70/30 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samples = len(y)\n",
    "indices = np.arange(total_samples)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_size = int(0.7 * total_samples)\n",
    "\n",
    "train_idx = indices[:train_size]\n",
    "test_idx = indices[train_size:]\n",
    "\n",
    "X_train = X_norm[train_idx]\n",
    "y_train = y[train_idx]\n",
    "\n",
    "X_test = X_norm[test_idx]\n",
    "y_test = y[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verification of rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train disease rate:\", y_train.mean())\n",
    "print(\"Test disease rate:\", y_test.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar rates in both sets indicate a balanced split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 Summary\n",
    "\n",
    "- Downloaded dataset from Kaggle (303 patient records)\n",
    "- Target: 1=disease present, 0=absent\n",
    "- Picked 6 features: age, cholesterol, BP, max HR, ST depression, vessels\n",
    "- No missing values found\n",
    "- Normalized features (helps gradient descent converge faster)\n",
    "- Split: 70% train, 30% test\n",
    "\n",
    "Ready to build the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Implement Basic Logistic Regression\n",
    "## Goal\n",
    "Build logistic regression from scratch to predict heart disease risk. Using gradient descent, no sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Sigmoid activation function\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output is a probability (0 to 1) representing disease risk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using binary cross-entropy as the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, weights, bias):\n",
    "    \"\"\"\n",
    "    Binary cross-entropy loss\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    z = X @ weights + bias\n",
    "    predictions = sigmoid(z)\n",
    "    \n",
    "    epsilon = 1e-8  # numerical stability\n",
    "    cost = -(1/m) * np.sum(\n",
    "        y * np.log(predictions + epsilon) +\n",
    "        (1 - y) * np.log(1 - predictions + epsilon)\n",
    "    )\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Penalizes wrong predictions, especially false negatives (missing sick patients)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(X, y, weights, bias):\n",
    "    \"\"\"\n",
    "    Compute gradients of cost w.r.t w and b\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    predictions = sigmoid(X @ weights + bias)\n",
    "    \n",
    "    grad_w = (1/m) * (X.T @ (predictions - y))\n",
    "    grad_b = (1/m) * np.sum(predictions - y)\n",
    "    \n",
    "    return grad_w, grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, learning_rate=0.01, iterations=2000):\n",
    "    \"\"\"\n",
    "    Train logistic regression using gradient descent\n",
    "    \"\"\"\n",
    "    m, n_features = X.shape\n",
    "    weights = np.zeros(n_features)\n",
    "    bias = 0.0\n",
    "    \n",
    "    cost_history = []\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        grad_w, grad_b = compute_gradients(X, y, weights, bias)\n",
    "        \n",
    "        weights -= learning_rate * grad_w\n",
    "        bias -= learning_rate * grad_b\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            cost = compute_cost(X, y, weights, bias)\n",
    "            cost_history.append(cost)\n",
    "    \n",
    "    return weights, bias, cost_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alpha=0.01 works well. Running 1000-3000 iterations ensures convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, bias, cost_history = gradient_descent(\n",
    "    X_train, y_train,\n",
    "    learning_rate=0.01,\n",
    "    iterations=2500\n",
    ")\n",
    "\n",
    "print(\"Final weights:\", weights)\n",
    "print(\"Final bias:\", bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(cost_history)\n",
    "plt.xlabel(\"Iterations (x50)\")\n",
    "plt.ylabel(\"Cost (Binary Cross-Entropy)\")\n",
    "plt.title(\"Training Loss Convergence\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost drops smoothly - GD is working. No oscillation means learning rate is good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, weights, bias, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Predict class labels using learned parameters\n",
    "    \"\"\"\n",
    "    probabilities = sigmoid(X @ weights + bias)\n",
    "    return (probabilities >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_metrics(y_true, y_pred):\n",
    "    accuracy = np.mean(y_true == y_pred)\n",
    "    \n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    \n",
    "    precision = tp / (tp + fp + 1e-8)\n",
    "    recall = tp / (tp + fn + 1e-8)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "    \n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = predict(X_train, weights, bias)\n",
    "y_test_pred = predict(X_test, weights, bias)\n",
    "\n",
    "train_metrics = classification_metrics(y_train, y_train_pred)\n",
    "test_metrics = classification_metrics(y_test, y_test_pred)\n",
    "\n",
    "print(\"Train metrics (Acc, Prec, Recall, F1):\", train_metrics)\n",
    "print(\"Test metrics  (Acc, Prec, Recall, F1):\", test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics Explanation\n",
    "\n",
    "- Accuracy: overall correctness\n",
    "- Recall: how many sick patients we catch (most important for medical diagnosis)\n",
    "- Precision: when we predict disease, how often are we right\n",
    "- F1: balance between precision and recall\n",
    "\n",
    "Missing a sick patient (false negative) is worse than a false alarm, so recall matters more here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Visualize Decision Boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why visualize?\n",
    "\n",
    "Logistic regression draws a straight line (boundary) between classes. By plotting pairs of features, we can see:\n",
    "\n",
    "- Which features separate healthy vs sick patients\n",
    "- If a linear boundary makes sense\n",
    "- Where the model might struggle\n",
    "\n",
    "This helps understand what the model is doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function: Train 2D Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logistic_2d(X, y, alpha=0.01, iters=2000):\n",
    "    \"\"\"\n",
    "    Train logistic regression on 2D feature space\n",
    "    \"\"\"\n",
    "    w = np.zeros(2)\n",
    "    b = 0.0\n",
    "    \n",
    "    for _ in range(iters):\n",
    "        y_hat = sigmoid(X @ w + b)\n",
    "        dw = (1 / len(y)) * (X.T @ (y_hat - y))\n",
    "        db = (1 / len(y)) * np.sum(y_hat - y)\n",
    "        \n",
    "        w -= alpha * dw\n",
    "        b -= alpha * db\n",
    "        \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function: Plot Decision Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(X, y, w, b, feature_names):\n",
    "    \"\"\"\n",
    "    Plot data points and logistic regression decision boundary\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(6,5))\n",
    "    \n",
    "    # Scatter plot\n",
    "    plt.scatter(X[y==0, 0], X[y==0, 1], label=\"No Disease\", alpha=0.7)\n",
    "    plt.scatter(X[y==1, 0], X[y==1, 1], label=\"Disease\", alpha=0.7)\n",
    "    \n",
    "    # Decision boundary: w1*x1 + w2*x2 + b = 0\n",
    "    x_vals = np.linspace(X[:,0].min(), X[:,0].max(), 100)\n",
    "    y_vals = -(w[0] * x_vals + b) / w[1]\n",
    "    \n",
    "    plt.plot(x_vals, y_vals, color=\"black\", linestyle=\"--\", label=\"Decision Boundary\")\n",
    "    \n",
    "    plt.xlabel(feature_names[0])\n",
    "    plt.ylabel(feature_names[1])\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.title(f\"Decision Boundary: {feature_names[0]} vs {feature_names[1]}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Pair 1: Age vs Cholesterol\n",
    "Medical context\n",
    "\n",
    "- Age is a strong demographic risk factor\n",
    "\n",
    "- Cholesterol is directly linked to atherosclerosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_1 = [\"age\", \"chol\"]\n",
    "X_pair = data[features_1].values\n",
    "y_pair = data[\"target\"].values\n",
    "\n",
    "# Normalize\n",
    "X_pair = (X_pair - X_pair.mean(axis=0)) / X_pair.std(axis=0)\n",
    "\n",
    "w_1, b_1 = train_logistic_2d(X_pair, y_pair)\n",
    "\n",
    "plot_decision_boundary(X_pair, y_pair, w_1, b_1, features_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "- Lots of overlap between classes\n",
    "- Higher age + cholesterol correlates with disease\n",
    "- Linear boundary is limited here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Pair 2: Resting BP vs Max Heart Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_2 = [\"trestbps\", \"thalach\"]\n",
    "X_pair = data[features_2].values\n",
    "y_pair = data[\"target\"].values\n",
    "\n",
    "X_pair = (X_pair - X_pair.mean(axis=0)) / X_pair.std(axis=0)\n",
    "\n",
    "w_2, b_2 = train_logistic_2d(X_pair, y_pair)\n",
    "\n",
    "plot_decision_boundary(X_pair, y_pair, w_2, b_2, features_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "- Cleaner separation than age-cholesterol\n",
    "- Low max HR + high BP = higher risk\n",
    "- Linear model fits reasonably well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Pair 3: ST Depression vs Number of Vessels\n",
    "Medical context\n",
    "\n",
    "- ST depression (oldpeak) indicates exercise-induced ischemia\n",
    "\n",
    "- Number of vessels (ca) shows structural severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_3 = [\"oldpeak\", \"ca\"]\n",
    "X_pair = data[features_3].values\n",
    "y_pair = data[\"target\"].values\n",
    "\n",
    "X_pair = (X_pair - X_pair.mean(axis=0)) / X_pair.std(axis=0)\n",
    "\n",
    "w_3, b_3 = train_logistic_2d(X_pair, y_pair)\n",
    "\n",
    "plot_decision_boundary(X_pair, y_pair, w_3, b_3, features_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "- Best separation so far\n",
    "- More ST depression + vessels = disease\n",
    "- These features are highly informative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaway\n",
    "Some feature pairs (ST depression + vessels) separate well. Others (age + cholesterol) have too much overlap. Logistic regression works for linear patterns but can't capture complex/nonlinear relationships. For better performance, could try feature engineering or more complex models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Repeat with Regularization\n",
    "## Why?\n",
    "\n",
    "Small datasets (303 patients) with correlated features can cause overfitting. Regularization helps by:\n",
    "\n",
    "- Preventing model from being too complex\n",
    "- Making weights smaller/more stable\n",
    "- Better performance on new patients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_reg(X, y, w, b, lam):\n",
    "    \"\"\"\n",
    "    Binary cross-entropy with L2 regularization\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    y_hat = sigmoid(X @ w + b)\n",
    "    \n",
    "    epsilon = 1e-8\n",
    "    base_cost = -(1/m) * np.sum(\n",
    "        y * np.log(y_hat + epsilon) +\n",
    "        (1 - y) * np.log(1 - y_hat + epsilon)\n",
    "    )\n",
    "    \n",
    "    reg_term = (lam / (2*m)) * np.sum(w**2)\n",
    "    \n",
    "    return base_cost + reg_term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients_reg(X, y, w, b, lam):\n",
    "    \"\"\"\n",
    "    Gradients with L2 regularization\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    y_hat = sigmoid(X @ w + b)\n",
    "    \n",
    "    dw = (1/m) * (X.T @ (y_hat - y)) + (lam/m) * w\n",
    "    db = (1/m) * np.sum(y_hat - y)\n",
    "    \n",
    "    return dw, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent with Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_reg(X, y, alpha=0.01, iters=2000, lam=0.0):\n",
    "    \"\"\"\n",
    "    Logistic regression with L2 regularization\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    w = np.zeros(n)\n",
    "    b = 0.0\n",
    "    cost_history = []\n",
    "    \n",
    "    for i in range(iters):\n",
    "        dw, db = compute_gradients_reg(X, y, w, b, lam)\n",
    "        \n",
    "        w -= alpha * dw\n",
    "        b -= alpha * db\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            cost = compute_cost_reg(X, y, w, b, lam)\n",
    "            cost_history.append(cost)\n",
    "    \n",
    "    return w, b, cost_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Experiment: Different λ Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = [0, 0.001, 0.01, 0.1, 1]\n",
    "results = []\n",
    "\n",
    "for lam in lambdas:\n",
    "    w_reg, b_reg, _ = gradient_descent_reg(\n",
    "        X_train, y_train,\n",
    "        alpha=0.01,\n",
    "        iters=2500,\n",
    "        lam=lam\n",
    "    )\n",
    "    \n",
    "    y_test_pred = predict(X_test, w_reg, b_reg)\n",
    "    acc, prec, rec, f1 = classification_metrics(y_test, y_test_pred)\n",
    "    \n",
    "    weight_norm = np.linalg.norm(w_reg)\n",
    "    \n",
    "    results.append([lam, acc, prec, rec, f1, weight_norm])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization Results Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results_df = pd.DataFrame(\n",
    "    results,\n",
    "    columns=[\"Lambda\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"||w||\"]\n",
    ")\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Comparison Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "\n",
    "for lam in [0, 0.01, 0.1]:\n",
    "    _, _, cost_hist = gradient_descent_reg(\n",
    "        X_train, y_train,\n",
    "        alpha=0.01,\n",
    "        iters=2000,\n",
    "        lam=lam\n",
    "    )\n",
    "    plt.plot(cost_hist, label=f\"λ = {lam}\")\n",
    "\n",
    "plt.xlabel(\"Iterations (x50)\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.title(\"Effect of L2 Regularization on Convergence\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Boundary: Regularized vs Unregularized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"oldpeak\", \"ca\"]\n",
    "X_pair = data[features].values\n",
    "y_pair = data[\"target\"].values\n",
    "\n",
    "X_pair = (X_pair - X_pair.mean(axis=0)) / X_pair.std(axis=0)\n",
    "\n",
    "# Unregularized\n",
    "w0, b0 = train_logistic_2d(X_pair, y_pair)\n",
    "\n",
    "# Regularized\n",
    "wR, bR, _ = gradient_descent_reg(X_pair, y_pair, lam=0.1)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.scatter(X_pair[y_pair==0,0], X_pair[y_pair==0,1], label=\"No Disease\", alpha=0.6)\n",
    "plt.scatter(X_pair[y_pair==1,0], X_pair[y_pair==1,1], label=\"Disease\", alpha=0.6)\n",
    "\n",
    "x_vals = np.linspace(X_pair[:,0].min(), X_pair[:,0].max(), 100)\n",
    "plt.plot(x_vals, -(w0[0]*x_vals + b0)/w0[1], \"--\", label=\"Unregularized\")\n",
    "plt.plot(x_vals, -(wR[0]*x_vals + bR)/wR[1], label=\"Regularized (λ=0.1)\")\n",
    "\n",
    "plt.xlabel(\"ST Depression\")\n",
    "plt.ylabel(\"Number of Vessels\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.title(\"Effect of Regularization on Decision Boundary\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "- Higher λ → smaller weights\n",
    "- Boundaries become smoother\n",
    "- Moderate λ (0.01-0.1) works best\n",
    "- Too much λ → underfitting (model too simple)\n",
    "\n",
    "Best value seems to be λ around 0.01-0.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Explore Deployment in Amazon SageMaker\n",
    "## Why deploy?\n",
    "\n",
    "Training is just the first step. For real-world use, models need to:\n",
    "\n",
    "- Run in the cloud\n",
    "- Be accessible via API\n",
    "- Handle real-time requests\n",
    "\n",
    "SageMaker provides infrastructure for deploying ML models in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model parameters\n",
    "model_params = {\n",
    "    \"weights\": w_reg,\n",
    "    \"bias\": b_reg,\n",
    "    \"lambda\": 0.1\n",
    "}\n",
    "\n",
    "np.save(\"heart_lr_model.npy\", model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_patient(features, w, b):\n",
    "    \"\"\"\n",
    "    Predict heart disease risk for a single patient\n",
    "    \"\"\"\n",
    "    z = np.dot(features, w) + b\n",
    "    probability = sigmoid(z)\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example patient (normalized with training stats)\n",
    "sample_patient = np.array([60, 300, 140, 150, 2.3, 2])\n",
    "\n",
    "sample_patient = (sample_patient - X_train.mean(axis=0)) / X_train.std(axis=0)\n",
    "\n",
    "risk = predict_patient(sample_patient, w_reg, b_reg)\n",
    "\n",
    "print(f\"Predicted heart disease risk: {risk:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "- Output is between 0 and 1\n",
    "- Above 0.5 = high risk\n",
    "- Could be used in a clinical decision system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Execution\n",
    "\n",
    "Process:\n",
    "\n",
    "1. Opened SageMaker Studio\n",
    "2. Created notebook instance (Python 3)\n",
    "3. Uploaded heart_disease_lr_analysis.ipynb and heart.csv\n",
    "4. Ran all cells\n",
    "5. Checked plots and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Endpoint Output\n",
    "Example request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Age=60, Chol=300, BP=140, MaxHR=150, STDepression=2.3, Vessels=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predicted Probability = 0.68\n",
    "Risk Level: High"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison: Local vs SageMaker Execution\n",
    "| Aspect               | Local Execution | SageMaker Execution |\n",
    "| -------------------- | --------------- | ------------------- |\n",
    "| Environment          | Laptop          | Managed cloud       |\n",
    "| Reproducibility      | Manual          | High                |\n",
    "| Scalability          | Limited         | High                |\n",
    "| Cost                 | Free            | Free tier           |\n",
    "| Enterprise readiness | Low             | High                |\n",
    "\n",
    "\n",
    "- Observation\n",
    "Results and plots were identical, validating environment consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Thoughts\n",
    "\n",
    "This lab shows:\n",
    "\n",
    "- How to build logistic regression from scratch\n",
    "- Working with medical data\n",
    "- Deploying on cloud platforms\n",
    "- Even simple models can be useful in practice"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
